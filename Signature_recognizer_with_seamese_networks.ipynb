{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasgleria/signature-recognizer/blob/main/Signature_recognizer_with_seamese_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README - Projeto de Verificação de Similaridade de Assinaturas com Triplet Loss no CEDAR\n",
        "\n",
        "Este projeto avança na implementação de um sistema de verificação de similaridade de imagens, agora aplicado especificamente a **assinaturas manuscritas** utilizando redes neurais e a função de perda **Triplet Loss**. A base de dados utilizada para esta fase de protótipo é o desafiador **dataset CEDAR**, composto por assinaturas genuínas e forjadas.\n",
        "\n",
        "O objetivo central é treinar uma rede neural para aprender a gerar **embeddings** (vetores de características) para assinaturas. A meta é que assinaturas genuínas do mesmo indivíduo resultem em embeddings próximos no espaço vetorial, enquanto assinaturas de indivíduos diferentes (ou forjadas) produzam embeddings distantes. Isso possibilita, dada uma assinatura de \"consulta\", verificar sua autenticidade comparando-a com um conjunto de assinaturas de referência.\n",
        "\n",
        "## Fase de Protótipo\n",
        "\n",
        "A **Fase de Protótipo** estende os conceitos validados na fase de testes (com MNIST), focando na aplicação prática do sistema de similaridade para verificação de assinaturas. Aqui, você encontrará as implementações e lógicas para:\n",
        "\n",
        "---\n",
        "\n",
        "### Implementando o Ambiente e Configurações Iniciais\n",
        "\n",
        "Nesta seção, o ambiente do Google Colab é preparado. São realizadas as **instalações de bibliotecas essenciais**, como `timm` (para modelos de visão pré-treinados como o EfficientNet), `scikit-image` (para manipulação e pré-processamento de imagens) e `scikit-learn` (para métricas de avaliação). Além disso, são definidas **configurações globais** como o `BATCH_SIZE`, `Learning Rate (LR)`, o número de `EPOCHS` e a **seleção do dispositivo de processamento** (`cuda` para GPU ou `cpu`).\n",
        "\n",
        "---\n",
        "\n",
        "### Carregando e Preparando o Dataset CEDAR para Triplets\n",
        "\n",
        "Esta parte é crucial para adaptar o sistema ao novo domínio das assinaturas. O dataset **CEDAR** é baixado, descompactado e sua estrutura de arquivos é mapeada para identificar assinaturas genuínas e forjadas, bem como o ID de cada pessoa. Para a **Triplet Loss**, são gerados triplets (Anchor, Positive, Negative) com base nas assinaturas.\n",
        "* É implementada uma função auxiliar para **mapear os caminhos das imagens e os IDs das pessoas** do dataset CEDAR.\n",
        "* A classe **`APN_Signature_Dataset`** é desenvolvida. Diferente do MNIST, ela incorpora um **pré-processamento específico para assinaturas**, que inclui conversão para escala de cinza, **binarização por Otsu** e redimensionamento, garantindo que as imagens estejam no formato adequado (3 canais) para o EfficientNet.\n",
        "* Ao final, você poderá **visualizar um exemplo de triplet de assinatura** para validar o pré-processamento e a geração das amostras.\n",
        "\n",
        "---\n",
        "\n",
        "### Preparando DataLoaders e Definindo o Modelo\n",
        "\n",
        "Com o dataset CEDAR estruturado, os **`DataLoaders`** são configurados para otimizar o carregamento dos triplets de assinaturas em **batches** durante o treinamento e validação. A arquitetura do modelo (`APN_Model`) é definida, utilizando o **EfficientNet-B0** pré-treinado da biblioteca `timm` como *backbone*. A camada classificadora final do EfficientNet é ajustada para produzir os **vetores de embedding** com o tamanho desejado (512).\n",
        "\n",
        "---\n",
        "\n",
        "### Funções de Treinamento, Avaliação e Loop Principal\n",
        "\n",
        "Esta seção reutiliza as funções **`train_fn`** e **`eval_fn`** da fase de testes. A `train_fn` executa o passo de treinamento com a **Triplet Loss**, retropropagação e atualização de pesos via otimizador **Adam**. A `eval_fn` avalia o desempenho no conjunto de validação. O **loop de treinamento** orquestra essas funções, monitora a `valid_loss` e **salva os pesos do modelo** (`best_signature_model.pt`) que apresentar o melhor desempenho no conjunto de validação.\n",
        "\n",
        "---\n",
        "\n",
        "### Geração de Embeddings e Verificação de Similaridade\n",
        "\n",
        "Após o treinamento do modelo com assinaturas, esta seção foca na sua aplicação para a tarefa de verificação:\n",
        "* Uma nova classe **`InferenceSignatureDataset`** é criada, otimizada para carregar imagens e seus metadados (ID da pessoa, caminho) para a fase de inferência.\n",
        "* A função **`get_signature_encodings`** é adaptada para gerar eficientemente os embeddings de todas as assinaturas (genuínas e forjadas), criando um banco de dados de referência.\n",
        "* A função **`euclidean_dist`** é utilizada para calcular a distância entre embeddings.\n",
        "* Uma função **`verify_signature`** é implementada. Ela recebe uma assinatura de consulta, seu ID de pessoa e um banco de embeddings de referência, e retorna se a assinatura é **genuína** (distância abaixo de um limiar) ou **falsificada** (distância acima do limiar).\n",
        "* Um **exemplo prático de verificação** é demonstrado, testando tanto assinaturas genuínas quanto falsificadas para uma pessoa específica.\n",
        "\n",
        "---\n",
        "\n",
        "### Avaliação de Desempenho: Curva ROC e EER\n",
        "\n",
        "Esta seção finaliza a avaliação quantitativa do sistema de verificação de assinaturas, utilizando métricas padrão de biometria:\n",
        "* São calculadas as distâncias entre pares **Anchor-Positive** (considerados genuínos) e **Anchor-Negative** (considerados falsificados) do `valid_df`.\n",
        "* A **Curva ROC (Receiver Operating Characteristic)** é gerada e plotada, visualizando o trade-off entre a Taxa de Falsa Aceitação (FAR) e a Taxa de Verdadeiros Positivos (TPR) em diferentes limiares. A **Área Sob a Curva (AUC)** é calculada, indicando a capacidade geral de distinção do modelo.\n",
        "* É calculado o **Equal Error Rate (EER)**, o ponto de operação onde a Taxa de Falsa Aceitação (FAR) e a Taxa de Falsa Rejeição (FRR) são iguais. O **limiar de distância** correspondente ao EER é identificado, sendo este o ponto de operação otimizado para o sistema.\n",
        "* Os resultados do EER e AUC são impressos, e o ponto EER é marcado na Curva ROC plotada.\n",
        "* O exemplo de verificação de similaridade é **re-executado utilizando o limiar otimizado do EER**, demonstrando a aplicação prática dessa métrica."
      ],
      "metadata": {
        "id": "ukWCL9sIzx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalações\n",
        "\n",
        "# Instale as bibliotecas essenciais para o projeto.\n",
        "- **'segmentation-models-pytorch'** é uma biblioteca poderosa para tarefas de segmentação.\n",
        "- **'albumentations'** é utilizada para aumento de dados (data augmentation) em tempo real, o que ajuda a melhorar a robustez e generalização do modelo.\n",
        "- **'opencv-contrib-python'** é fundamental para operações de processamento de imagem, e a atualização garante que tenhamos as funcionalidades mais recentes."
      ],
      "metadata": {
        "id": "jwje1avqqZB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "!pip install --upgrade opencv-contrib-python"
      ],
      "metadata": {
        "id": "CEErzWCXQREE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fe8d3c-e50a-43ec-b4ea-021ac07e5592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-contrib-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baixando o dataset"
      ],
      "metadata": {
        "id": "rRZKmzJXrUse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Crie um diretório para o dataset se ainda não existir\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Mude para o diretório de dados\n",
        "%cd data\n",
        "\n",
        "# Baixe o arquivo zip\n",
        "!wget https://github.com/nikostsagk/signature-verification/releases/download/cedar/cedar_dataset.zip\n",
        "\n",
        "# Volte para o diretório raiz do Colab (opcional, dependendo de onde você quer trabalhar)\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTdZsksZmC3O",
        "outputId": "89439eb0-dbd7-45bc-bd01-c5367475b80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "--2025-06-09 18:59:32--  https://github.com/nikostsagk/signature-verification/releases/download/cedar/cedar_dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/220312682/95ebb580-019c-11ea-8af8-61acae883803?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250609%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250609T185932Z&X-Amz-Expires=300&X-Amz-Signature=c368f37ebbe01bb5d9048324be2714a2fa968e3bd6512b954b966b72bfbfa373&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcedar_dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-06-09 18:59:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/220312682/95ebb580-019c-11ea-8af8-61acae883803?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250609%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250609T185932Z&X-Amz-Expires=300&X-Amz-Signature=c368f37ebbe01bb5d9048324be2714a2fa968e3bd6512b954b966b72bfbfa373&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcedar_dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 254168735 (242M) [application/octet-stream]\n",
            "Saving to: ‘cedar_dataset.zip.1’\n",
            "\n",
            "cedar_dataset.zip.1 100%[===================>] 242.39M  96.8MB/s    in 2.5s    \n",
            "\n",
            "2025-06-09 18:59:35 (96.8 MB/s) - ‘cedar_dataset.zip.1’ saved [254168735/254168735]\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decompactando o dataset"
      ],
      "metadata": {
        "id": "O42A-tADrdTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data/cedar_dataset.zip -d /content/CEDAR/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXH2ad_jnBL6",
        "outputId": "5ad00ee4-db75-4945-c6de-26fa36e0e4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data/cedar_dataset.zip\n",
            "replace /content/CEDAR/full_org/original_29_18.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importações"
      ],
      "metadata": {
        "id": "CNmYrDiirjCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importe as bilbiotecas necessárias para o desenvolvimento do modelo"
      ],
      "metadata": {
        "id": "ePDegMg_rlKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importações adicionais necessárias para esta seção"
      ],
      "metadata": {
        "id": "s0yo9Xnmr3T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Para operações com o sistema de arquivos\n",
        "import glob # Para encontrar arquivos que correspondem a um padrão específico\n",
        "import pandas as pd # Para manipulação de dados em formato de DataFrame\n",
        "import numpy as np # Para operações numéricas\n",
        "from sklearn.model_selection import train_test_split # Para dividir o dataset em conjuntos de treino e teste\n",
        "import random # Para operações de seleção aleatória\n",
        "from tqdm import tqdm # Para exibir barras de progresso"
      ],
      "metadata": {
        "id": "4-8zxB3hnbxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração dos diretórios do CEDAR"
      ],
      "metadata": {
        "id": "IinDODmVsOgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CEDAR_ROOT_DIR = '/content/CEDAR' # Define o diretório base do dataset CEDAR.\n",
        "                                   # Ajuste este caminho se você descompactou o dataset em um local diferente.\n",
        "\n",
        "# Define o caminho completo para o diretório que contém as assinaturas originais (genuínas).\n",
        "ORIGINAL_DIR = os.path.join(CEDAR_ROOT_DIR, 'full_org')\n",
        "\n",
        "# Define o caminho completo para o diretório que contém as assinaturas forjadas.\n",
        "FORGED_DIR = os.path.join(CEDAR_ROOT_DIR, 'full_forg')"
      ],
      "metadata": {
        "id": "ie_ELoiBnegZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapeando imagens e gerando o CSV de Triplets"
      ],
      "metadata": {
        "id": "gsq6zyL8tuV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths_and_labels_corrected(root_dir, is_forgery):\n",
        "    \"\"\"\n",
        "    Coleta caminhos de imagem, IDs de pessoa e tipo de assinatura (genuína/falsificada),\n",
        "    baseando-se na estrutura de nomes de arquivo \"original_IDPESSOA_NUM.png\" ou \"forgeries_IDPESSOA_NUM.png\".\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    # Assumimos que os arquivos estão diretamente dentro de root_dir (full_org ou full_forg)\n",
        "    # e seguem o padrão original_ID_NUM.png ou forgeries_ID_NUM.png\n",
        "\n",
        "    # Exemplo de regex ou split para extrair ID da pessoa\n",
        "    # original_10_1.png  -> Person ID: 10\n",
        "    # forgeries_10_1.png -> Person ID: 10\n",
        "\n",
        "    # Usar glob para encontrar todos os arquivos .png\n",
        "    for img_file in glob.glob(os.path.join(root_dir, '*.png')):\n",
        "        img_name = os.path.basename(img_file) # Ex: \"original_10_1.png\"\n",
        "\n",
        "        parts = img_name.split('_')\n",
        "        if len(parts) >= 3:\n",
        "            try:\n",
        "                # O ID da pessoa é a segunda parte do nome (ex: '10' em 'original_10_1.png')\n",
        "                person_id = int(parts[1])\n",
        "                data.append({\n",
        "                    'image_path': img_file,\n",
        "                    'person_id': person_id,\n",
        "                    'is_forgery': is_forgery\n",
        "                })\n",
        "            except ValueError:\n",
        "                print(f\"Aviso: Não foi possível extrair o ID da pessoa de {img_name}. Ignorando.\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"Aviso: Nome de arquivo inesperado: {img_name}. Ignorando.\")\n",
        "            continue\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def create_triplets_df(original_df, forged_df, samples_per_original=5):\n",
        "    \"\"\"\n",
        "    Cria um DataFrame de triplets (Anchor, Positive, Negative) para o treinamento.\n",
        "    Anchor: Original da pessoa X\n",
        "    Positive: Outra Original da pessoa X\n",
        "    Negative: Falsificação da pessoa X OU Original/Falsificação da pessoa Y\n",
        "    \"\"\"\n",
        "    triplets = []\n",
        "    all_person_ids = sorted(original_df['person_id'].unique())\n",
        "\n",
        "    # Agrupar imagens por pessoa para facilitar a seleção\n",
        "    original_by_person = {pid: df for pid, df in original_df.groupby('person_id')}\n",
        "    forged_by_person = {pid: df for pid, df in forged_df.groupby('person_id')}\n",
        "\n",
        "    for person_id in tqdm(all_person_ids, desc=\"Gerando Triplets\"):\n",
        "        person_originals = original_by_person.get(person_id, pd.DataFrame())\n",
        "        person_forgeries = forged_by_person.get(person_id, pd.DataFrame())\n",
        "\n",
        "        if person_originals.empty:\n",
        "            continue\n",
        "\n",
        "        for _, anchor_row in person_originals.iterrows():\n",
        "            anchor_path = anchor_row['image_path']\n",
        "\n",
        "            # Geração de 'samples_per_original' triplets para cada anchor\n",
        "            for _ in range(samples_per_original):\n",
        "                # Selecionar Positive: Outra original da mesma pessoa\n",
        "                positive_candidates = person_originals[person_originals['image_path'] != anchor_path]\n",
        "                if not positive_candidates.empty:\n",
        "                    positive_path = random.choice(positive_candidates['image_path'].tolist())\n",
        "                else: # Se não há outras originais, pulamos este anchor ou geramos apenas pares A-N\n",
        "                    continue\n",
        "\n",
        "                # Selecionar Negative: Priorizar falsificações do mesmo usuário,\n",
        "                # caso contrário, usar qualquer assinatura de outra pessoa.\n",
        "                negative_path = None\n",
        "                if not person_forgeries.empty:\n",
        "                    negative_path = random.choice(person_forgeries['image_path'].tolist())\n",
        "                else:\n",
        "                    # Se não houver falsificações para essa pessoa, escolher uma de outra pessoa\n",
        "                    other_person_ids = [pid for pid in all_person_ids if pid != person_id]\n",
        "                    if other_person_ids:\n",
        "                        chosen_other_person_id = random.choice(other_person_ids)\n",
        "\n",
        "                        # Tenta pegar uma falsificação de outra pessoa primeiro\n",
        "                        other_person_forgeries = forged_by_person.get(chosen_other_person_id, pd.DataFrame())\n",
        "                        if not other_person_forgeries.empty:\n",
        "                            negative_path = random.choice(other_person_forgeries['image_path'].tolist())\n",
        "                        else:\n",
        "                            # Se não houver falsificações nem para outras pessoas, pega uma original de outra pessoa\n",
        "                            other_person_originals = original_by_person.get(chosen_other_person_id, pd.DataFrame())\n",
        "                            if not other_person_originals.empty:\n",
        "                                negative_path = random.choice(other_person_originals['image_path'].tolist())\n",
        "\n",
        "                if negative_path:\n",
        "                    triplets.append({\n",
        "                        'Anchor': anchor_path,\n",
        "                        'Positive': positive_path,\n",
        "                        'Negative': negative_path\n",
        "                    })\n",
        "    return pd.DataFrame(triplets)\n",
        "\n",
        "\n",
        "# --- Usar a função ---\n",
        "original_df = get_image_paths_and_labels_corrected(ORIGINAL_DIR, is_forgery=False)\n",
        "forged_df = get_image_paths_and_labels_corrected(FORGED_DIR, is_forgery=True)\n",
        "\n",
        "print(f\"Total de assinaturas genuínas encontradas: {len(original_df)}\")\n",
        "print(f\"Total de assinaturas falsificadas encontradas: {len(forged_df)}\")\n",
        "\n",
        "# Criar o DataFrame de triplets\n",
        "cedar_triplets_df = create_triplets_df(original_df, forged_df, samples_per_original=10)\n",
        "print(f\"Total de triplets gerados: {len(cedar_triplets_df)}\")\n",
        "\n",
        "# Salvar o CSV (opcional, mas recomendado para reuso)\n",
        "CEDAR_CSV_FILE = 'cedar_triplets.csv'\n",
        "cedar_triplets_df.to_csv(CEDAR_CSV_FILE, index=False)\n",
        "print(f\"CSV de triplets salvo em: {CEDAR_CSV_FILE}\")\n",
        "\n",
        "# Dividir em treino e validação\n",
        "train_df, valid_df = train_test_split(cedar_triplets_df, test_size=0.20, random_state=42)\n",
        "\n",
        "print(f\"Tamanho do dataset de treino CEDAR: {len(train_df)}\")\n",
        "print(f\"Tamanho do dataset de validação CEDAR: {len(valid_df)}\")"
      ],
      "metadata": {
        "id": "sk25M3Zinf1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação e Carregamento do Dataset CEDAR para Treinamento\n"
      ],
      "metadata": {
        "id": "7_zc66DeykeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta seção, adaptamos as classes de Dataset e DataLoader para trabalhar com o dataset CEDAR, que contém imagens de assinaturas. A preparação das imagens de assinatura requer etapas específicas de pré-processamento para garantir que estejam no formato correto e otimizadas para o modelo."
      ],
      "metadata": {
        "id": "iKmP8pD-uN60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Biblioteca principal do PyTorch para construção de redes neurais.\n",
        "from torch.utils.data import Dataset, DataLoader # Classes para criar datasets e carregar dados em batches.\n",
        "from skimage import io, color, filters, transform # scikit-image: Para leitura de imagens, conversão de cor, filtros e transformações.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Para visualização de imagens.\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuração ---\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0001 # Learning rate pode ser menor para fine-tuning\n",
        "EPOCHS = 10 # Define o número de épocas de treinamento.\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Determina qual dispositivo será usado para o treinamento (GPU se disponível, caso contrário CPU).\n",
        "IMAGE_SIZE = (224, 224) # Tamanho padrão de entrada para EfficientNet\n",
        "\n",
        "class APN_Signature_Dataset(Dataset):\n",
        "   \"\"\"\n",
        "    Dataset customizado para carregar triplets (Anchor, Positive, Negative)\n",
        "    de imagens de assinatura do dataset CEDAR, com pré-processamento específico.\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "      \"\"\"\n",
        "        Inicializa o dataset.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame contendo os caminhos das imagens\n",
        "                               para Anchor, Positive e Negative.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "       \"\"\"\n",
        "        Retorna o número total de triplets no dataset.\n",
        "        \"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retorna um triplet de imagens pré-processadas dado um índice.\n",
        "\n",
        "        Args:\n",
        "            idx (int): O índice do triplet no DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Um triplet de tensores de imagem (Anchor, Positive, Negative).\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Caminhos das imagens\n",
        "        A_path = row.Anchor\n",
        "        P_path = row.Positive\n",
        "        N_path = row.Negative\n",
        "\n",
        "        # Carregar e pré-processar imagens\n",
        "        A_img = self._preprocess_signature(A_path)\n",
        "        P_img = self._preprocess_signature(P_path)\n",
        "        N_img = self._preprocess_signature(N_path)\n",
        "\n",
        "        return A_img, P_img, N_img\n",
        "\n",
        "    def _preprocess_signature(self, img_path):\n",
        "        \"\"\"\n",
        "        Carrega uma imagem de assinatura, aplica pré-processamento (conversão para\n",
        "        escala de cinza, binarização, redimensionamento) e a converte em um tensor.\n",
        "\n",
        "        Args:\n",
        "            img_path (str): Caminho completo para o arquivo da imagem.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: O tensor da imagem pré-processada e formatada para o modelo.\n",
        "        \"\"\"\n",
        "        img = io.imread(img_path)\n",
        "\n",
        "        # Converter para escala de cinza se não for (algumas imagens TIFF podem ser CMYK, RGB, etc.)\n",
        "        if len(img.shape) == 3 and img.shape[2] == 3: # RGB\n",
        "            img = color.rgb2gray(img)\n",
        "        elif len(img.shape) == 3 and img.shape[2] == 4: # RGBA\n",
        "            img = color.rgba2rgb(img)\n",
        "            img = color.rgb2gray(img)\n",
        "        # Se já for grayscale (2D) ou tiver um único canal (altura, largura, 1), continua\n",
        "\n",
        "        # Binarização usando o método de Otsu:\n",
        "        # O método de Otsu encontra automaticamente um limiar para separar pixels de fundo e primeiro plano.\n",
        "        thresh = filters.threshold_otsu(img)\n",
        "        # Aplica o limiar: pixels acima do limiar se tornam True (fundo), abaixo se tornam False (tinta).\n",
        "        binary_img = img > thresh\n",
        "        # Inverte a imagem binarizada: agora a tinta é 1 (clara) e o fundo é 0 (escuro).\n",
        "        # Isso é feito para que a \"informação\" (tinta) seja representada por valores mais altos,\n",
        "        # o que pode ser mais consistente com como os modelos aprendem características.\n",
        "        binary_img = np.invert(binary_img).astype(np.float32)\n",
        "\n",
        "        # Redimensiona a imagem para o tamanho definido em IMAGE_SIZE.\n",
        "        # `anti_aliasing=True` ajuda a suavizar as bordas da imagem após o redimensionamento.\n",
        "        resized_img = transform.resize(binary_img, IMAGE_SIZE, anti_aliasing=True)\n",
        "\n",
        "        # Converte o array NumPy redimensionado para um tensor PyTorch.\n",
        "        # .unsqueeze(0) adiciona uma dimensão de canal no início (agora é 1, H, W para imagem em escala de cinza).\n",
        "        img_tensor = torch.from_numpy(resized_img).unsqueeze(0)\n",
        "\n",
        "        # Replicar o canal para 3 canais (RGB):\n",
        "        # A maioria dos modelos pré-treinados (como EfficientNet) espera 3 canais de entrada (RGB).\n",
        "        # Replicamos o único canal da imagem de assinatura 3 vezes para atender a essa expectativa.\n",
        "        img_tensor = img_tensor.repeat(3, 1, 1)\n",
        "\n",
        "        return img_tensor\n",
        "\n",
        "# --- Criar instâncias do dataset adaptado ---\n",
        "trainset = APN_Signature_Dataset(train_df)\n",
        "validset = APN_Signature_Dataset(valid_df)\n",
        "\n",
        "# Imprime o tamanho dos datasets para verificar se foram carregados corretamente.\n",
        "print(f\"\\nSize of trainset (CEDAR) : {len(trainset)}\")\n",
        "print(f\"Size of validset (CEDAR) : {len(validset)}\")\n",
        "\n",
        "# --- Visualizar um exemplo de triplet de assinatura ---\n",
        "idx = 0 # Seleciona o primeiro triplet para visualização.\n",
        "A_sig, P_sig, N_sig = trainset[idx] # Obtém o triplet de tensores.\n",
        "\n",
        "# Cria uma figura com 1 linha e 3 colunas para exibir as três imagens.\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "# Exibe a imagem Anchor.\n",
        "# .permute(1, 2, 0) muda a ordem dos canais para (H, W, C) para o matplotlib.\n",
        "# [:,:,0] pega apenas um canal para exibição, já que os 3 canais são réplicas.\n",
        "ax1.set_title('Anchor Signature')\n",
        "ax1.imshow(A_sig.permute(1, 2, 0)[:,:,0].cpu().numpy(), cmap='gray')\n",
        "ax1.axis('off') # Remove os eixos para uma visualização mais limpa.\n",
        "\n",
        "# Exibe a imagem Positive.\n",
        "ax2.set_title('Positive Signature')\n",
        "ax2.imshow(P_sig.permute(1, 2, 0)[:,:,0].cpu().numpy(), cmap='gray')\n",
        "ax2.axis('off')\n",
        "\n",
        "# Exibe a imagem Negative.\n",
        "ax3.set_title('Negative Signature')\n",
        "ax3.imshow(N_sig.permute(1, 2, 0)[:,:,0].cpu().numpy(), cmap='gray')\n",
        "ax3.axis('off')\n",
        "plt.show() # Mostra a figura.\n",
        "\n",
        "# --- Carregar Dataset em batches ---\n",
        "# Cria DataLoaders para o conjunto de treinamento e validação.\n",
        "# 'batch_size=BATCH_SIZE' define o número de imagens por lote.\n",
        "# 'shuffle=True' para o treino embaralha os dados em cada época.\n",
        "# 'num_workers=2' permite o carregamento de dados em paralelo, acelerando o processo (pode ser ajustado).\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "validloader = DataLoader(validset, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "# Imprime o número de batches em cada DataLoader.\n",
        "print(f\"\\nNo. of batches in trainloader (CEDAR) : {len(trainloader)}\")\n",
        "print(f\"No. of batches in validloader (CEDAR) : {len(validloader)}\")\n",
        "\n",
        "# Pega um batch de exemplo para verificar o formato dos tensores de imagem.\n",
        "for A, P, N in trainloader:\n",
        "    break # Sai do loop após pegar o primeiro batch.\n",
        "print(f\"One image batch shape (CEDAR) : {A.shape}\") # Imprime o formato do tensor das imagens Anchor."
      ],
      "metadata": {
        "id": "F2CTwlLNoKrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição do Modelo, Funções de Treinamento e Loop Principal"
      ],
      "metadata": {
        "id": "TIedmkrSxLSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta seção, reutilizamos a mesma arquitetura de modelo e as funções de treinamento e avaliação estabelecidas na fase de testes do MNIST. A principal diferença aqui é que o modelo agora será treinado com o dataset CEDAR (assinaturas), o que o capacitará para a tarefa de verificação de similaridade de assinaturas."
      ],
      "metadata": {
        "id": "XgejLsILxR4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm # Biblioteca para modelos de imagem pré-treinados.\n",
        "import torch.nn.functional as F # Funções de ativação e perda comuns do PyTorch.\n",
        "from torch import nn # Módulo principal para a construção de redes neurais.\n",
        "\n",
        "class APN_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a arquitetura do modelo para gerar embeddings.\n",
        "    Utiliza o EfficientNet-B0 pré-treinado como backbone e uma camada linear\n",
        "    para projetar as características no espaço de embedding.\n",
        "    \"\"\"\n",
        "    def _init_(self, emb_size=512):\n",
        "        \"\"\"\n",
        "        Inicializa o modelo.\n",
        "        Args:\n",
        "            emb_size (int): O tamanho do vetor de embedding de saída.\n",
        "        \"\"\"\n",
        "        super(APN_Model, self)._init_()\n",
        "        # Carrega o EfficientNet-B0 pré-treinado.\n",
        "        # Ele espera 3 canais de entrada, que já tratamos no APN_Signature_Dataset.\n",
        "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "        # Ajusta a camada classificadora final para o tamanho de embedding desejado.\n",
        "        self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Define o passo forward do modelo.\n",
        "        Args:\n",
        "            images (torch.Tensor): Tensor de imagens de entrada.\n",
        "        Returns:\n",
        "            torch.Tensor: Vetores de embedding gerados pelo modelo.\n",
        "        \"\"\"\n",
        "        embeddings = self.efficientnet(images)\n",
        "        return embeddings\n",
        "\n",
        "# Instancia o modelo e o move para o dispositivo (GPU ou CPU).\n",
        "model = APN_Model()\n",
        "model.to(DEVICE)\n",
        "print(f\"Modelo carregado no dispositivo: {DEVICE}\")\n",
        "\n",
        "def train_fn(model, dataloader, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Função para realizar uma época de treinamento do modelo.\n",
        "    Processa os batches, calcula a Triplet Loss, realiza a retropropagação\n",
        "    e atualiza os pesos do modelo.\n",
        "    \"\"\"\n",
        "    model.train() # Coloca o modelo em modo de treinamento.\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for A, P, N in tqdm(dataloader, desc=\"Treinando\"): # Itera sobre os batches do dataloader.\n",
        "        # Move os tensores (Anchor, Positive, Negative) para o dispositivo configurado.\n",
        "        A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
        "\n",
        "        # Gera os embeddings para cada imagem do triplet.\n",
        "        A_embs = model(A)\n",
        "        P_embs = model(P)\n",
        "        N_embs = model(N)\n",
        "\n",
        "        # Calcula a Triplet Loss com base nos embeddings.\n",
        "        loss = criterion(A_embs, P_embs, N_embs)\n",
        "\n",
        "        optimizer.zero_grad() # Zera os gradientes acumulados.\n",
        "        loss.backward()       # Realiza a retropropagação para calcular os gradientes.\n",
        "        optimizer.step()      # Atualiza os pesos do modelo.\n",
        "\n",
        "        total_loss += loss.item() # Acumula a perda do batch.\n",
        "\n",
        "    return total_loss / len(dataloader) # Retorna a perda média da época.\n",
        "\n",
        "def eval_fn(model, dataloader, criterion):\n",
        "    \"\"\"\n",
        "    Função para avaliar o modelo no conjunto de validação.\n",
        "    Calcula a perda sem atualizar os pesos do modelo.\n",
        "    \"\"\"\n",
        "    model.eval() # Coloca o modelo em modo de avaliação (desativa dropout, etc.).\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad(): # Desativa o cálculo de gradientes para otimizar a inferência.\n",
        "        for A, P, N in tqdm(dataloader, desc=\"Validando\"):\n",
        "            A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
        "\n",
        "            A_embs = model(A)\n",
        "            P_embs = model(P)\n",
        "            N_embs = model(N)\n",
        "\n",
        "            loss = criterion(A_embs, P_embs, N_embs)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader) # Retorna a perda média da época de validação.\n",
        "\n",
        "# Define a função de perda TripletMarginLoss.\n",
        "criterion = nn.TripletMarginLoss()\n",
        "# Define o otimizador Adam, que será usado para ajustar os pesos do modelo.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# Inicializa a melhor perda de validação com infinito.\n",
        "best_valid_loss = np.inf\n",
        "\n",
        "print(\"\\nIniciando o treinamento com CEDAR...\")\n",
        "for i in range(EPOCHS): # Loop principal de treinamento, iterando sobre o número de épocas.\n",
        "    train_loss = train_fn(model, trainloader, optimizer, criterion) # Executa uma época de treinamento.\n",
        "    valid_loss = eval_fn(model, validloader, criterion)             # Executa uma época de validação.\n",
        "\n",
        "    # Verifica se a perda de validação atual é a melhor já obtida.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        # Se for, salva os pesos do modelo.\n",
        "        torch.save(model.state_dict(), 'best_signature_model.pt')\n",
        "        best_valid_loss = valid_loss # Atualiza a melhor perda de validação.\n",
        "        print(\"SALVOS_PESOS_SUCESSO\") # Confirma que os pesos foram salvos.\n",
        "\n",
        "    # Imprime a perda de treino e validação para a época atual.\n",
        "    print(f\"ÉPOCA: {i+1} Loss Treino: {train_loss:.4f} Loss Validação: {valid_loss:.4f}\")\n",
        "\n",
        "# Após o treinamento, carrega os pesos do modelo que teve o melhor desempenho na validação.\n",
        "model.load_state_dict(torch.load('best_signature_model.pt'))\n",
        "print(\"\\nMelhor modelo de assinatura carregado para inferência.\")"
      ],
      "metadata": {
        "id": "12Sf7uU-oN2k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Geração de Embeddings e Verificação de Similaridade\n"
      ],
      "metadata": {
        "id": "v93jR0anxk0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa final da fase de protótipo, focamos na inferência. Utilizaremos o modelo treinado para gerar embeddings para todas as assinaturas (genuínas e falsificadas) do dataset CEDAR. Com esses embeddings em mãos, poderemos realizar a verificação de similaridade, comparando uma assinatura de consulta com um conjunto de referências para determinar se ela é considerada genuína ou forjada para um determinado indivíduo."
      ],
      "metadata": {
        "id": "9LnB8jm7xowz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class APN_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a arquitetura do modelo para gerar embeddings.\n",
        "    Utiliza o EfficientNet-B0 pré-treinado como backbone e uma camada linear\n",
        "    para projetar as características no espaço de embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_size=512):\n",
        "        super(APN_Model, self).__init__()\n",
        "        # Carrega o EfficientNet-B0 pré-treinado.\n",
        "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "        # Substitui a camada classificadora final para produzir embeddings do tamanho desejado.\n",
        "        self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Define o passo forward do modelo, que transforma imagens em embeddings.\n",
        "        \"\"\"\n",
        "        embeddings = self.efficientnet(images)\n",
        "        return embeddings\n",
        "\n",
        "# Criar uma instância do modelo e carregá-lo.\n",
        "model = APN_Model()\n",
        "model.to(DEVICE) # Move o modelo para a GPU ou CPU.\n",
        "try:\n",
        "    # Tenta carregar os pesos do modelo previamente salvo.\n",
        "    model.load_state_dict(torch.load('best_signature_model.pt'))\n",
        "    print(\"Modelo 'best_signature_model.pt' carregado com sucesso.\")\n",
        "except FileNotFoundError:\n",
        "    # Emite um aviso se o arquivo de pesos não for encontrado.\n",
        "    print(\"Aviso: 'best_signature_model.pt' não encontrado. O modelo não foi carregado a partir de um checkpoint. Certifique-se de que o treinamento foi executado e o modelo foi salvo.\")\n",
        "    # Se o modelo não foi salvo, você pode precisar treiná-lo antes de tentar gerar embeddings.\n",
        "\n",
        "\n",
        "# --- Função de Pré-processamento de Imagem (mantém a mesma do APN_Signature_Dataset) ---\n",
        "def _preprocess_signature(img_path):\n",
        "    \"\"\"\n",
        "    Carrega uma imagem de assinatura, aplica pré-processamento (conversão para\n",
        "    escala de cinza, binarização, redimensionamento) e a converte em um tensor.\n",
        "    Esta função é uma cópia da lógica de pré-processamento usada no `APN_Signature_Dataset`\n",
        "    para garantir consistência.\n",
        "    \"\"\"\n",
        "    img = io.imread(img_path)\n",
        "\n",
        "    # Converte a imagem para escala de cinza se necessário.\n",
        "    if len(img.shape) == 3 and img.shape[2] == 3: # RGB\n",
        "        img = color.rgb2gray(img)\n",
        "    elif len(img.shape) == 3 and img.shape[2] == 4: # RGBA\n",
        "        img = color.rgba2rgb(img)\n",
        "        img = color.rgb2gray(img)\n",
        "\n",
        "    # Binariza a imagem usando o método de Otsu para separar o primeiro plano (tinta) do fundo.\n",
        "    thresh = filters.threshold_otsu(img)\n",
        "    binary_img = img > thresh # Fundo=True/1, Tinta=False/0\n",
        "    binary_img = np.invert(binary_img).astype(np.float32) # Inverte para Tinta=1, Fundo=0.\n",
        "\n",
        "    # Redimensiona a imagem para o tamanho esperado pelo modelo.\n",
        "    resized_img = transform.resize(binary_img, IMAGE_SIZE, anti_aliasing=True)\n",
        "\n",
        "    # Converte o array NumPy para tensor PyTorch e adiciona a dimensão de canal (1, H, W).\n",
        "    img_tensor = torch.from_numpy(resized_img).unsqueeze(0)\n",
        "    # Replica o canal único para 3 canais, conforme exigido por modelos pré-treinados como EfficientNet.\n",
        "    img_tensor = img_tensor.repeat(3, 1, 1)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "# --- NOVA CLASSE DE DATASET PARA INFERÊNCIA ---\n",
        "class InferenceSignatureDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset otimizado para a fase de inferência.\n",
        "    Ele recebe um DataFrame com caminhos de imagem e IDs de pessoa,\n",
        "    e retorna o tensor da imagem pré-processada, o ID da pessoa e o caminho original.\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.df = df # DataFrame contendo 'image_path' e 'person_id'.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row['image_path'] # Acessa o caminho da imagem da linha atual.\n",
        "        person_id = row['person_id'] # Acessa o ID da pessoa da linha atual.\n",
        "\n",
        "        img_tensor = _preprocess_signature(img_path) # Pré-processa a imagem.\n",
        "        # Retorna o tensor da imagem, o ID da pessoa (como Tensor para agrupamento no DataLoader)\n",
        "        # e o caminho da imagem (como string).\n",
        "        return img_tensor, torch.tensor(person_id, dtype=torch.long), img_path\n",
        "\n",
        "# --- Adaptação da função get_signature_encodings para usar InferenceSignatureDataset ---\n",
        "def get_signature_encodings(model, df_images):\n",
        "    \"\"\"\n",
        "    Gera embeddings para todas as imagens contidas em um DataFrame de metadados.\n",
        "    Args:\n",
        "        model (nn.Module): O modelo treinado para gerar embeddings.\n",
        "        df_images (pd.DataFrame): DataFrame com colunas 'image_path' e 'person_id'.\n",
        "    Returns:\n",
        "        pd.DataFrame: Um DataFrame contendo os embeddings gerados, o caminho da imagem\n",
        "                      e o ID da pessoa.\n",
        "    \"\"\"\n",
        "    # Cria uma instância do novo dataset de inferência.\n",
        "    signature_dataset_inference = InferenceSignatureDataset(df_images)\n",
        "    # Cria um DataLoader para processar as imagens em batches para a inferência.\n",
        "    signature_dataloader_inference = DataLoader(signature_dataset_inference, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    encodings = []      # Lista para armazenar os vetores de embedding.\n",
        "    image_paths = []    # Lista para armazenar os caminhos das imagens.\n",
        "    person_ids = []     # Lista para armazenar os IDs das pessoas.\n",
        "\n",
        "    model.eval() # Coloca o modelo em modo de avaliação (desativa dropout, etc.).\n",
        "    with torch.no_grad(): # Desativa o cálculo de gradientes para otimização da inferência.\n",
        "        for imgs_batch, pids_batch, paths_batch in tqdm(signature_dataloader_inference, desc=\"Gerando embeddings de assinatura\"):\n",
        "            imgs_batch = imgs_batch.to(DEVICE) # Move o lote de imagens para o dispositivo.\n",
        "            encs_batch = model(imgs_batch)     # Gera os embeddings.\n",
        "\n",
        "            # Converte os embeddings para NumPy na CPU e os adiciona à lista.\n",
        "            encodings.extend(encs_batch.squeeze().cpu().detach().numpy())\n",
        "            # Converte os IDs das pessoas para NumPy na CPU e os adiciona à lista.\n",
        "            person_ids.extend(pids_batch.cpu().numpy())\n",
        "            # Adiciona os caminhos das imagens à lista.\n",
        "            image_paths.extend(paths_batch)\n",
        "\n",
        "    # Cria um DataFrame Pandas com os embeddings e informações adicionais.\n",
        "    df_enc_signatures = pd.DataFrame(encodings)\n",
        "    df_enc_signatures['image_path'] = image_paths\n",
        "    df_enc_signatures['person_id'] = person_ids\n",
        "    return df_enc_signatures\n",
        "\n",
        "\n",
        "# --- Re-executar a coleta e criação do df de original_df e forged_df (se não tiver feito ainda) ---\n",
        "# Este bloco assegura que 'original_df' e 'forged_df' (DataFrames com metadados das imagens)\n",
        "# estejam disponíveis no ambiente. Se você já os criou em etapas anteriores e o ambiente não\n",
        "# foi resetado, pode pular esta re-execução.\n",
        "#\n",
        "# --- Configuração dos diretórios do CEDAR (ajuste conforme seu upload) ---\n",
        "# Define os caminhos raiz do dataset CEDAR.\n",
        "CEDAR_ROOT_DIR = '/content/CEDAR' # Ou onde você descompactou o dataset\n",
        "ORIGINAL_DIR = os.path.join(CEDAR_ROOT_DIR, 'full_org') # Caminho para assinaturas originais.\n",
        "FORGED_DIR = os.path.join(CEDAR_ROOT_DIR, 'full_forg') # Caminho para assinaturas falsificadas.\n",
        "\n",
        "def get_image_paths_and_labels_corrected(root_dir, is_forgery):\n",
        "    \"\"\"\n",
        "    Função auxiliar para coletar caminhos de imagem e IDs de pessoa,\n",
        "    lendo a estrutura de arquivos do dataset CEDAR.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    # Itera sobre arquivos PNG dentro do diretório.\n",
        "    for img_file in glob.glob(os.path.join(root_dir, '*.png')): # Mude para '*.tif' se for o caso do seu dataset.\n",
        "        img_name = os.path.basename(img_file)\n",
        "        parts = img_name.split('_')\n",
        "        if len(parts) >= 3:\n",
        "            try:\n",
        "                person_id = int(parts[1]) # Extrai o ID da pessoa.\n",
        "                data.append({\n",
        "                    'image_path': img_file,\n",
        "                    'person_id': person_id,\n",
        "                    'is_forgery': is_forgery\n",
        "                })\n",
        "            except ValueError:\n",
        "                print(f\"Aviso: Não foi possível extrair o ID da pessoa de {img_name}. Ignorando.\")\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"Aviso: Nome de arquivo inesperado: {img_name}. Ignorando.\")\n",
        "            continue\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Cria os DataFrames para assinaturas originais e falsificadas.\n",
        "original_df = get_image_paths_and_labels_corrected(ORIGINAL_DIR, is_forgery=False)\n",
        "forged_df = get_image_paths_and_labels_corrected(FORGED_DIR, is_forgery=True)\n",
        "\n",
        "print(f\"Total de assinaturas genuínas encontradas: {len(original_df)}\")\n",
        "print(f\"Total de assinaturas falsificadas encontradas: {len(forged_df)}\")\n",
        "# Fim do bloco de re-execução\n",
        "\n",
        "\n",
        "# --- Agora, o seu código original de inferência deve funcionar ---\n",
        "# Gera os embeddings para todas as assinaturas genuínas e falsificadas.\n",
        "all_original_signatures_enc = get_signature_encodings(model, original_df)\n",
        "all_forged_signatures_enc = get_signature_encodings(model, forged_df)\n",
        "\n",
        "print(\"\\nEmbeddings de assinaturas genuínas gerados:\")\n",
        "print(all_original_signatures_enc.head())\n",
        "print(\"\\nEmbeddings de assinaturas falsificadas gerados:\")\n",
        "print(all_forged_signatures_enc.head())\n",
        "\n",
        "# --- Função de Distância (Mantém a mesma) ---\n",
        "def euclidean_dist(img_enc, ref_enc_arr):\n",
        "    \"\"\"\n",
        "    Calcula a distância euclidiana entre um embedding de imagem (consulta)\n",
        "    e um array de embeddings de referência.\n",
        "    \"\"\"\n",
        "    # Garante que ambos os inputs são 2D para a operação (1, emb_size) ou (n, emb_size).\n",
        "    img_enc = img_enc.reshape(1, -1) if img_enc.ndim == 1 else img_enc\n",
        "    ref_enc_arr = ref_enc_arr.reshape(1, -1) if ref_enc_arr.ndim == 1 else ref_enc_arr\n",
        "    # Calcula a norma L2 (distância euclidiana) ao longo do eixo das features.\n",
        "    dist = np.linalg.norm(img_enc - ref_enc_arr, axis=1)\n",
        "    return dist\n",
        "\n",
        "# --- Funções para Verificação de Assinatura (Mantém a mesma) ---\n",
        "def verify_signature(query_img_path, query_person_id, model, reference_embeddings_df, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Verifica se uma assinatura de consulta é genuína para uma determinada pessoa,\n",
        "    comparando-a com embeddings de referência.\n",
        "    Args:\n",
        "        query_img_path (str): Caminho da imagem da assinatura de consulta.\n",
        "        query_person_id (int): ID da pessoa a quem a assinatura supostamente pertence.\n",
        "        model (nn.Module): O modelo treinado.\n",
        "        reference_embeddings_df (pd.DataFrame): DataFrame com embeddings de assinaturas genuínas de referência.\n",
        "        threshold (float): Limiar de distância para considerar uma assinatura como genuína.\n",
        "    Returns:\n",
        "        tuple: (bool) True se a assinatura for genuína, False caso contrário;\n",
        "               (float) a distância mínima encontrada.\n",
        "    \"\"\"\n",
        "    # Pré-processa a imagem de consulta usando a função de pré-processamento.\n",
        "    query_img_tensor = _preprocess_signature(query_img_path)\n",
        "    query_img_tensor = query_img_tensor.to(DEVICE) # Move o tensor para o dispositivo.\n",
        "\n",
        "    model.eval() # Coloca o modelo em modo de avaliação.\n",
        "    with torch.no_grad(): # Desativa o cálculo de gradientes.\n",
        "        # Gera o embedding da imagem de consulta. .unsqueeze(0) adiciona uma dimensão de batch.\n",
        "        query_enc = model(query_img_tensor.unsqueeze(0))\n",
        "        # Move o embedding para a CPU e converte para NumPy.\n",
        "        query_enc = query_enc.detach().cpu().numpy()\n",
        "\n",
        "    # Filtra os embeddings de referência para a pessoa em questão.\n",
        "    person_reference_encs = reference_embeddings_df[\n",
        "        reference_embeddings_df['person_id'] == query_person_id\n",
        "    ].iloc[:, :-2].to_numpy() # Seleciona apenas as colunas de embedding.\n",
        "\n",
        "    if person_reference_encs.shape[0] == 0:\n",
        "        print(f\"Aviso: Nenhuma assinatura de referência encontrada para a pessoa {query_person_id}. Não é possível verificar.\")\n",
        "        return False, np.inf # Retorna falso e distância infinita se não houver referências.\n",
        "\n",
        "    # Calcula as distâncias euclidianas entre o embedding de consulta e todas as referências.\n",
        "    distances = euclidean_dist(query_enc, person_reference_encs)\n",
        "    min_distance = np.min(distances) # Pega a distância mínima.\n",
        "\n",
        "    # Determina se a assinatura é genuína com base no limiar.\n",
        "    is_genuine = min_distance < threshold\n",
        "    return is_genuine, min_distance\n",
        "\n",
        "# --- Exemplo de Verificação ---\n",
        "# Define um ID de pessoa para testar a verificação.\n",
        "test_person_id = 1\n",
        "# Obtém os caminhos das assinaturas genuínas para a pessoa de teste.\n",
        "genuine_signatures_for_test_person = original_df[original_df['person_id'] == test_person_id]['image_path'].tolist()\n",
        "\n",
        "if genuine_signatures_for_test_person:\n",
        "    # Seleciona aleatoriamente uma assinatura genuína para testar.\n",
        "    genuine_query_path = random.choice(genuine_signatures_for_test_person)\n",
        "    # Realiza a verificação.\n",
        "    is_genuine, min_dist = verify_signature(genuine_query_path, test_person_id, model, all_original_signatures_enc, threshold=0.5)\n",
        "    print(f\"\\nVerificando assinatura GENUÍNA de Pessoa {test_person_id} ({os.path.basename(genuine_query_path)}):\")\n",
        "    print(f\"Distância Mínima: {min_dist:.4f}, É Genuína: {is_genuine}\")\n",
        "\n",
        "# Obtém os caminhos das assinaturas falsificadas para a pessoa de teste.\n",
        "forged_signatures_for_test_person = forged_df[forged_df['person_id'] == test_person_id]['image_path'].tolist()\n",
        "\n",
        "if forged_signatures_for_test_person:\n",
        "    # Seleciona aleatoriamente uma assinatura falsificada para testar.\n",
        "    forged_query_path = random.choice(forged_signatures_for_test_person)\n",
        "    # Realiza a verificação.\n",
        "    is_genuine, min_dist = verify_signature(forged_query_path, test_person_id, model, all_original_signatures_enc, threshold=0.5)\n",
        "    print(f\"\\nVerificando assinatura FALSIFICADA de Pessoa {test_person_id} ({os.path.basename(forged_query_path)}):\")\n",
        "    print(f\"Distância Mínima: {min_dist:.4f}, É Genuína: {is_genuine}\")\n",
        "else:\n",
        "    print(f\"\\nNão há falsificações para a Pessoa {test_person_id} para testar.\")\n",
        "    # Caso não haja falsificações para o usuário de teste, tenta testar com uma assinatura\n",
        "    # genuína de outra pessoa para ver se é corretamente classificada como não genuína.\n",
        "    other_person_id = test_person_id + 1\n",
        "    # Garante que 'other_person_id' existe e é diferente do 'test_person_id'.\n",
        "    if other_person_id not in all_original_signatures_enc['person_id'].unique():\n",
        "        other_person_id = next(iter(all_original_signatures_enc['person_id'].unique())) # Pega o primeiro ID diferente.\n",
        "        if other_person_id == test_person_id:\n",
        "            print(\"Aviso: Apenas uma pessoa encontrada para teste.\")\n",
        "        else:\n",
        "            other_person_genuine_signatures = original_df[original_df['person_id'] == other_person_id]['image_path'].tolist()\n",
        "            if other_person_genuine_signatures:\n",
        "                other_person_query_path = random.choice(other_person_genuine_signatures)\n",
        "                is_genuine, min_dist = verify_signature(other_person_query_path, test_person_id, model, all_original_signatures_enc, threshold=0.5)\n",
        "                print(f\"\\nVerificando assinatura GENUÍNA de OUTRA Pessoa ({other_person_id}) como FALSIFICADA para Pessoa {test_person_id}:\")\n",
        "                print(f\"Distância Mínima: {min_dist:.4f}, É Genuína: {is_genuine}\")"
      ],
      "metadata": {
        "id": "HEdmBXDXLn_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação de Desempenho: Curva ROC e EER\n"
      ],
      "metadata": {
        "id": "i3Qk6bylyTJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta seção, realizaremos uma avaliação quantitativa do sistema de verificação de assinaturas. Utilizaremos métricas padrão em sistemas de biometria: a Curva ROC (Receiver Operating Characteristic) e a Taxa de Erro Igual (Equal Error Rate - EER). A Curva ROC nos ajuda a visualizar o trade-off entre as taxas de falsos positivos e verdadeiros positivos em diferentes limiares, enquanto o EER identifica o ponto de operação onde as taxas de erro de falsa aceitação e falsa rejeição são iguais, fornecendo um limiar de decisão otimizado.\n",
        "\n"
      ],
      "metadata": {
        "id": "FIe8A6QeyXN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc # Funções para calcular a curva ROC e a Área Sob a Curva (AUC).\n",
        "import matplotlib.pyplot as plt # Para plotagem de gráficos.\n",
        "\n",
        "# --- Preparar o conjunto de teste para avaliação EER/ROC ---\n",
        "# Usaremos o 'valid_df' que já contém triplets de Anchor, Positive, Negative.\n",
        "# Para o cálculo de EER/ROC, precisamos de pares (distância, rótulo verdadeiro).\n",
        "# Rótulo verdadeiro: 0 para \"genuíno\" (par Anchor-Positive), 1 para \"falsificado\" (par Anchor-Negative).\n",
        "\n",
        "distances_genuine = [] # Lista para armazenar as distâncias entre pares Anchor e Positive (genuínos).\n",
        "distances_forged = []  # Lista para armazenar as distâncias entre pares Anchor e Negative (falsificados).\n",
        "\n",
        "# Assumimos que 'model', 'valid_df', 'DEVICE' e '_preprocess_signature' estão definidos\n",
        "# e que 'euclidean_dist' também está disponível no escopo.\n",
        "\n",
        "# Gerar embeddings para o conjunto de validação (se ainda não fez)\n",
        "print(\"\\nGerando embeddings para avaliação (Anchor, Positive, Negative de valid_df)...\")\n",
        "model.eval() # Define o modelo em modo de avaliação para desativar dropout e otimizar a inferência.\n",
        "\n",
        "# Dicionário para armazenar embeddings já calculados.\n",
        "# Isso evita recalcular embeddings para a mesma imagem se ela aparecer em múltiplos triplets,\n",
        "# otimizando o processo.\n",
        "embedding_cache = {}\n",
        "\n",
        "def get_embedding(img_path, model, device, cache):\n",
        "    \"\"\"\n",
        "    Função auxiliar para obter o embedding de uma imagem, utilizando um cache\n",
        "    para evitar reprocessamento desnecessário.\n",
        "    \"\"\"\n",
        "    if img_path in cache: # Verifica se o embedding já está no cache.\n",
        "        return cache[img_path]\n",
        "\n",
        "    # Se não estiver no cache, pré-processa a imagem e gera o embedding.\n",
        "    img_tensor = _preprocess_signature(img_path)\n",
        "    img_tensor = img_tensor.to(device)\n",
        "    with torch.no_grad(): # Desativa o cálculo de gradientes.\n",
        "        # Gera o embedding, remove a dimensão de batch e move para CPU/NumPy.\n",
        "        embedding = model(img_tensor.unsqueeze(0)).squeeze().cpu().numpy()\n",
        "    cache[img_path] = embedding # Armazena o embedding no cache.\n",
        "    return embedding\n",
        "\n",
        "# Itera sobre cada triplet no DataFrame de validação para calcular as distâncias.\n",
        "for _, row in tqdm(valid_df.iterrows(), total=len(valid_df), desc=\"Calculando distâncias para avaliação\"):\n",
        "    anchor_path = row['Anchor']\n",
        "    positive_path = row['Positive']\n",
        "    negative_path = row['Negative']\n",
        "\n",
        "    # Obtém os embeddings para as três imagens do triplet, utilizando a função otimizada com cache.\n",
        "    anchor_enc = get_embedding(anchor_path, model, DEVICE, embedding_cache)\n",
        "    positive_enc = get_embedding(positive_path, model, DEVICE, embedding_cache)\n",
        "    negative_enc = get_embedding(negative_path, model, DEVICE, embedding_cache)\n",
        "\n",
        "    # Distância para pares genuínos (Anchor, Positive): esperam-se distâncias baixas.\n",
        "    dist_ap = euclidean_dist(anchor_enc, positive_enc)[0] # [0] para pegar o valor escalar da distância.\n",
        "    distances_genuine.append(dist_ap)\n",
        "\n",
        "    # Distância para pares falsificados (Anchor, Negative): esperam-se distâncias altas.\n",
        "    dist_an = euclidean_dist(anchor_enc, negative_enc)[0]\n",
        "    distances_forged.append(dist_an)\n",
        "\n",
        "# Converter as listas de distâncias para arrays numpy para facilitar o processamento.\n",
        "distances_genuine = np.array(distances_genuine)\n",
        "distances_forged = np.array(distances_forged)\n",
        "\n",
        "# Combinar todas as distâncias e seus rótulos verdadeiros para o cálculo da ROC.\n",
        "# Rótulo 0 para genuíno (distância menor é melhor para esta classe), 1 para falsificado (distância maior é melhor para esta classe).\n",
        "# Para `roc_curve`, o `y_true` (rótulos verdadeiros) é binário (0 ou 1),\n",
        "# e `y_score` (scores de predição) deve ser tal que scores mais altos correspondam à classe \"positiva\" (rótulo 1).\n",
        "# Como distâncias altas correspondem à classe \"falsificado\" (nosso rótulo 1), podemos usar as distâncias diretamente como scores.\n",
        "\n",
        "all_distances = np.concatenate((distances_genuine, distances_forged))\n",
        "# Cria os rótulos verdadeiros: zeros para as distâncias genuínas, uns para as distâncias falsificadas.\n",
        "y_true = np.concatenate((np.zeros_like(distances_genuine), np.ones_like(distances_forged)))\n",
        "\n",
        "# Calcula a Curva ROC.\n",
        "# fpr: Taxa de Falsos Positivos (False Positive Rate)\n",
        "# tpr: Taxa de Verdadeiros Positivos (True Positive Rate)\n",
        "# thresholds: Os diferentes limiares de distância testados.\n",
        "fpr, tpr, thresholds = roc_curve(y_true, all_distances)\n",
        "roc_auc = auc(fpr, tpr) # Calcula a Área Sob a Curva ROC.\n",
        "\n",
        "# Calcular o Equal Error Rate (EER)\n",
        "# EER é o ponto onde a Taxa de Falsa Aceitação (FAR) é igual à Taxa de Falsa Rejeição (FRR).\n",
        "# FAR = FPR\n",
        "# FRR = 1 - TPR (já que TPR é a Taxa de Verdadeiros Positivos e FPR é a Taxa de Falsos Positivos)\n",
        "# Portanto, buscamos o ponto onde 1 - TPR = FPR.\n",
        "eer_threshold = None\n",
        "min_diff = np.inf # Inicializa a diferença mínima com infinito.\n",
        "eer_far = 0.0\n",
        "eer_frr = 0.0\n",
        "\n",
        "# Itera sobre os limiares para encontrar o EER.\n",
        "for i, threshold in enumerate(thresholds):\n",
        "    far = fpr[i]\n",
        "    frr = 1 - tpr[i] # Taxa de Falsos Negativos (FNR) também pode ser chamada de FRR.\n",
        "    diff = abs(far - frr) # Calcula a diferença absoluta entre FAR e FRR.\n",
        "    if diff < min_diff:\n",
        "        min_diff = diff\n",
        "        eer_threshold = threshold # Limiar onde FAR e FRR são mais próximos.\n",
        "        eer_far = far         # FAR no ponto EER.\n",
        "        eer_frr = frr         # FRR no ponto EER.\n",
        "\n",
        "print(f\"\\nResultados de Avaliação:\")\n",
        "print(f\"Número de pares genuínos: {len(distances_genuine)}\")\n",
        "print(f\"Número de pares falsificados: {len(distances_forged)}\")\n",
        "print(f\"EER (Equal Error Rate) Threshold: {eer_threshold:.4f}\")\n",
        "print(f\"FAR (False Acceptance Rate) no EER: {eer_far:.4f}\")\n",
        "print(f\"FRR (False Rejection Rate) no EER: {eer_frr:.4f}\")\n",
        "print(f\"AUC (Area Under Curve) da Curva ROC: {roc_auc:.4f}\")\n",
        "\n",
        "# Plotar Curva ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Plota a curva ROC.\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (área = {roc_auc:.2f})')\n",
        "# Plota a linha de classificação aleatória (diagonal).\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "# Plota o ponto EER na curva ROC.\n",
        "plt.scatter(eer_far, 1 - eer_frr, marker='o', color='red', s=100, label=f'EER (Threshold={eer_threshold:.2f}, FAR/FRR={eer_far:.2f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsa Aceitação (FAR)') # Ou False Positive Rate (FPR)\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos (TPR) - (1 - FRR)') # Ou True Positive Rate (TPR)\n",
        "plt.title('Curva ROC para Verificação de Assinatura')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Re-executar o exemplo de verificação com o EER Threshold otimizado ---\n",
        "print(\"\\n--- Verificação com o Limiar Otimizado (EER Threshold) ---\")\n",
        "threshold_optimized = eer_threshold # Usa o limiar de EER encontrado para as verificações.\n",
        "\n",
        "if genuine_signatures_for_test_person:\n",
        "    genuine_query_path = random.choice(genuine_signatures_for_test_person)\n",
        "    is_genuine, min_dist = verify_signature(genuine_query_path, test_person_id, model, all_original_signatures_enc, threshold=threshold_optimized)\n",
        "    print(f\"\\nVerificando assinatura GENUÍNA de Pessoa {test_person_id} ({os.path.basename(genuine_query_path)}):\")\n",
        "    print(f\"Distância Mínima: {min_dist:.4f}, É Genuína: {is_genuine} (Limiar: {threshold_optimized:.4f})\")\n",
        "\n",
        "if forged_signatures_for_test_person:\n",
        "    forged_query_path = random.choice(forged_signatures_for_test_person)\n",
        "    is_genuine, min_dist = verify_signature(forged_query_path, test_person_id, model, all_original_signatures_enc, threshold=threshold_optimized)\n",
        "    print(f\"\\nVerificando assinatura FALSIFICADA de Pessoa {test_person_id} ({os.path.basename(forged_query_path)}):\")\n",
        "    print(f\"Distância Mínima: {min_dist:.4f}, É Genuína: {is_genuine} (Limiar: {threshold_optimized:.4f})\")\n",
        "else:\n",
        "    print(f\"\\nNão há falsificações para a Pessoa {test_person_id} para testar com o limiar otimizado.\")\n",
        "    # Fallback para outra pessoa (como antes) para demonstrar a rejeição de não-genuínas.\n",
        "    other_person_id = test_person_id + 1\n",
        "    if other_person_id not in all_original_signatures_enc['person_id'].unique():\n",
        "        other_person_id = next(iter(all_original_signatures_enc['person_id'].unique()))\n",
        "\n",
        "    if other_person_id != test_person_id:\n",
        "        other_person_genuine_signatures = original_df[original_df['person_id'] == other_person_id]['image_path'].tolist()\n",
        "        if other_person_genuine_signatures:\n",
        "            other_person_query_path = random.choice(other_person_genuine_signatures)\n",
        "            is_genuine, min_dist = verify_signature(other_person_query_path, test_person_id, model, all_original_signatures_enc, threshold=threshold_optimized)\n",
        "            print(f\"\\nVerificando assinatura GENUÍNA de OUTRA Pessoa ({other_person_id}) como FALSIFICADA para Pessoa {test_person_id}:\")\n",
        "            print(f\"Distância Mínima: {min_dist:.4f}, É Genuína: {is_genuine} (Limiar: {threshold_optimized:.4f})\")"
      ],
      "metadata": {
        "id": "v4E8sQOf8mbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}